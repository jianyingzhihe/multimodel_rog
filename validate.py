from modelscope import AutoModelForCausalLM, AutoTokenizer


# æ¨¡å‹è·¯å¾„
model_path = "/root/autodl-tmp/RoG/qwen/output/v11-20250710-145738/checkpoint-201"

# åŠ è½½æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype="auto",
    device_map="auto",
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

# ç³»ç»Ÿæç¤ºè¯
with open("../remp/story.txt", "r", encoding="utf-8") as f:
    sys_prompt = f.read()

# å¯¹è¯å¾ªç¯
while True:
    try:
        quest = input("User: ").strip()
        if not quest:
            break

        # æ„å»º chat æ¶ˆæ¯ç»“æ„
        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": quest}
        ]

        # ä½¿ç”¨ Qwen çš„ chat æ¨¡æ¿æ„é€ è¾“å…¥
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # âœ… æ³¨æ„ï¼šè¿™é‡Œä¸è¦åŠ  [] æŠŠ text å˜æˆåˆ—è¡¨ï¼Œé™¤éä½ è¦æ‰¹é‡å¤„ç†
        model_inputs = tokenizer(text, return_tensors="pt").to(model.device)

        # ç”Ÿæˆå›å¤
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=512
        )

        # å»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆå†…å®¹
        generated_ids = generated_ids[:, model_inputs['input_ids'].shape[-1]:]

        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print(f"Assistant: {response}")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¨ç†å·²æ‰‹åŠ¨ä¸­æ–­ã€‚")
        break