import json
import os
import argparse
import tqdm
from PIL import Image
import google.generativeai as genai
from dataloader import dataf
from concurrent.futures import ThreadPoolExecutor, as_completed
# è®¾ç½® API å¯†é’¥
genai.configure(api_key="your_api_key")

# å…¨å±€æ¨¡å‹å˜é‡
model = None

# Gemini å›¾åƒ + æ–‡æœ¬æ¨ç†å‡½æ•°
def gemini_vision_query(image_path, question):
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Error loading image {image_path}: {e}")
        return ""

    # Gemini API è°ƒç”¨
    try:
        response = model.generate_content([image, question])
        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {e}")
        return ""

# å•ä¸ªæ ·æœ¬çš„å¤„ç†å‡½æ•°
def process_item(item):
    qid = item.id
    image_path = item.image
    question = item.question

    print(f"Processing sample ID: {qid}")
    print(f"Question: {question}")
    print(f"Image path: {image_path}")

    answer = gemini_vision_query(image_path, question)
    
    # æ„å»ºè¾“å‡ºå­—å…¸
    output_dict = {
        "id": qid,
        "question": question,
        "answer": answer
    }
    
    print(f"Answer: {answer}")
    print("=" * 50)

    return output_dict

# å¤šçº¿ç¨‹æ¨ç†å‡½æ•°
def generate_with_gemini(dataset, output_path, max_workers=2):
    processed_ids = []
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¯»å–å·²å¤„ç†çš„ID
    if os.path.exists(output_path):
        with open(output_path) as f:
            for line in f:
                temp = json.loads(line)
                id = temp["id"]
                processed_ids.append(id)
        print(f"Found {len(processed_ids)} already processed items")
    else:
        print("Output file does not exist, starting from scratch")

    # è¿‡æ»¤å‡ºæœªå¤„ç†çš„é¡¹ç›®
    unprocessed_items = [item for item in dataset.val if item.id not in processed_ids]
    print(f"Processing {len(unprocessed_items)} remaining items")
    
    if len(unprocessed_items) == 0:
        print("All items have been processed!")
        return

    with open(output_path, 'a', encoding='utf-8') as f:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_item = {
                executor.submit(process_item, item): item for item in unprocessed_items
            }

            for future in tqdm.tqdm(as_completed(future_to_item), total=len(future_to_item), desc="Processing with Gemini"):
                try:
                    result = future.result()
                    if result:
                        # å®æ—¶å†™å…¥ç»“æœï¼Œç¡®ä¿ä¸ä¼šä¸¢å¤±æ•°æ®
                        f.write(json.dumps(result, ensure_ascii=False) + "\n")
                        f.flush()
                except Exception as e:
                    item = future_to_item[future]
                    print(f"Error processing item {item.id}: {e}")

    print(f"âœ… Results saved to {output_path}")

def generate_with_gemini_single(dataset, output_path):
    """
    ä½¿ç”¨ Gemini API å¯¹æ•°æ®é›†ä¸­çš„å•ä¸ªæ ·æœ¬è¿›è¡Œè§†è§‰æ¨ç†æµ‹è¯•ï¼Œå¹¶å°†ç»“æœä¿å­˜ä¸º JSONLã€‚
    """
    processed_ids = []
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¯»å–å·²å¤„ç†çš„ID
    if os.path.exists(output_path):
        with open(output_path) as f:
            for line in f:
                temp = json.loads(line)
                id = temp["id"]
                processed_ids.append(id)
        print(f"Found {len(processed_ids)} already processed items")
    else:
        print("Output file does not exist, starting from scratch")
    
    with open(output_path, 'a', encoding='utf-8') as f:
        for item in dataset.combined:
            id = item.id
            if id in processed_ids:
                print(f"Skipping already processed item: {id}")
                continue
                
            result = process_item(item)
            
            # å†™å…¥ JSONL
            f.write(json.dumps(result, ensure_ascii=False) + "\n")
            f.flush()
            
            break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Gemini vision model validation script")
    parser.add_argument("--model", type=str, default="gemini-2.5-flash", 
                       choices=["gemini-1.5-flash", "gemini-2.5-pro", "gemini-2.5-flash"],
                       help="Gemini model to use")
    parser.add_argument("--dataset_type", type=str, default="fvqa", 
                       choices=["fvqa"],
                       help="Type of dataset to use")
    parser.add_argument("--qapath", type=str, default="new_dataset_release/new_dataset_release/all_qs_dict_release.json",
                       help="Path to QA data file")
    parser.add_argument("--image_dir", type=str, default="new_dataset_release/new_dataset_release/images",
                       help="Path to images directory")
    parser.add_argument("--test_single", action="store_true", 
                       help="Run single sample test instead of full processing")
    parser.add_argument("--max_workers", type=int, default=16,
                       help="Maximum number of worker threads for parallel processing")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = genai.GenerativeModel(args.model)
    print(f"ğŸ¤– Initialized model: {args.model}")
    
    # åŠ è½½æ•°æ®é›†
    dataset = dataf(args.qapath, args.image_dir)
    print(f"ğŸ“Š Loaded dataset: {args.dataset_type}")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å (åŸºäºæ¨¡å‹å’Œæ•°æ®é›†åç§°)
    model_name_clean = args.model.replace(".", "_").replace("-", "_")
    
    if args.test_single:
        # å•æ ·æœ¬æµ‹è¯•
        outputdir = f"single_test_result_{model_name_clean}_{args.dataset_type}.jsonl"
        print(f"ğŸ§ª Running single sample test with {args.model}...")
        generate_with_gemini_single(dataset, outputdir)
        print(f"âœ… Single sample test result saved to {outputdir}")
    else:
        # å…¨é‡å¹¶è¡Œå¤„ç†
        outputdir = f"output_{model_name_clean}_{args.dataset_type}_parallel.jsonl"
        print(f"ğŸš€ Running full dataset processing with {args.model}...")
        print(f"ğŸ“ˆ Using {args.max_workers} worker threads")
        generate_with_gemini(dataset, outputdir, max_workers=args.max_workers)
        
        # å¯é€‰è¯„ä¼°
        print(f"\nğŸ“‹ To evaluate results, you can run:")
        print(f"dataset.evaluate_jsonl('{outputdir}')")
