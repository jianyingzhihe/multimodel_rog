# multimodal/qwenmod.py
import os
import torch
import warnings
import logging
from PIL import Image
from PIL.JpegImagePlugin import samplings
from transformers import Gemma3ForConditionalGeneration, AutoProcessor
from .dataloader import *
from .multi import BaseMultiModalModel
from vllm import LLM,SamplingParams
from vllm.sampling_params import BeamSearchParams

# è®¾ç½®æ—¥å¿—çº§åˆ«ä»¥å‡å°‘è­¦å‘Šä¿¡æ¯
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# å¿½ç•¥ç‰¹å®šçš„è­¦å‘Š
warnings.filterwarnings("ignore", message=".*generation flags.*")
warnings.filterwarnings("ignore", message=".*top_p.*")
warnings.filterwarnings("ignore", message=".*top_k.*")

class googlemod(BaseMultiModalModel):
    def _load_model(self,type="hf",max_tokens=512,allowed_local_media_path=None, use_auth_token=None, **kwargs):
        self.type=type
        self.modeltype="gemma"
        if type=="hf":
            print(os.getcwd())
            print(self.modelpath)
            
            # ä¼˜å…ˆä½¿ç”¨ Flash Attention 2 ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
            attention_methods = [
                ("flash_attention_2", "Flash Attention 2 (fastest option)"),
                ("sdpa", "SDPA attention (backup option)"),
                ("eager", "Eager attention (fallback option)")
            ]
            
            model_loaded = False
            for attn_type, description in attention_methods:
                if model_loaded:
                    break
                    
                print(f"Trying {description}...")
                try:
                    load_kwargs = {
                        "pretrained_model_name_or_path": self.modelpath,
                        "torch_dtype": torch.bfloat16,
                        "attn_implementation": attn_type,
                        "device_map": "auto",
                        "token": use_auth_token if use_auth_token else None,
                        "low_cpu_mem_usage": True,
                    }
                    
                    # Flash Attention 2 ç‰¹æ®Šé…ç½®
                    if attn_type == "flash_attention_2":
                        print("ğŸ”¥ Configuring Flash Attention 2 optimizations...")
                        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æé«˜å…¼å®¹æ€§
                        os.environ["FLASH_ATTENTION_SKIP_CUDA_CHECK"] = "1"
            
                        # æ·»åŠ ä¸€äº›é¢å¤–çš„å‚æ•°æ¥æé«˜ç¨³å®šæ€§
                        load_kwargs.update({
                            "trust_remote_code": True,
                            "use_safetensors": True,
                        })
                    
                    self.model = Gemma3ForConditionalGeneration.from_pretrained(**load_kwargs)
                    print(f"âœ… Successfully loaded with {attn_type}")
                    model_loaded = True
                    self.attention_type = attn_type
                    
                except Exception as e:
                    print(f"âŒ {attn_type} failed: {str(e)[:100]}...")
                    continue
            
            if not model_loaded:
                raise RuntimeError("Failed to load model with any attention implementation")
            
            self.processor = AutoProcessor.from_pretrained(self.modelpath, token=use_auth_token if use_auth_token else None)
        if type=="vllm":
            num_gpus = torch.cuda.device_count()
            self.sampling_params = SamplingParams(
                max_tokens=max_tokens,  # ä¿®æ”¹è¿™é‡Œï¼šå¢åŠ æœ€å¤§è¾“å‡º token æ•°é‡

            )
            vllm_kwargs = {
                "model": self.modelpath,
                "tensor_parallel_size": num_gpus,
                "trust_remote_code": True,
                "tokenizer_mode": "auto",
                "dtype": torch.bfloat16,
                "enable_prefix_caching": True,
                "gpu_memory_utilization": 0.9,
                "allowed_local_media_path": allowed_local_media_path or "/root/autodl-tmp/RoG/qwen/data/OKVQA/val2014",
                "limit_mm_per_prompt": {"image": 1,"video": 0},
                "max_model_len": 4096,
                "max_num_seqs": 2
            }
            if use_auth_token:
                vllm_kwargs["hf_token"] = use_auth_token
            
            self.model = LLM(**vllm_kwargs)

    def inf_question_image(self, question: str, image: str):
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": question}
                ]
            }
        ]
        return self.inf_with_messages(messages)

    def inf_with_messages(self, messages: list):
        if self.type=="hf":
            inputs = self.processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=True,
                return_dict=True, return_tensors="pt"
            ).to(self.model.device, dtype=torch.bfloat16)
            input_len = inputs["input_ids"].shape[-1]
            
            with torch.inference_mode():
                # æ ¹æ® attention ç±»å‹ä¼˜åŒ–ç”Ÿæˆå‚æ•°
                generation_kwargs = {
                    'max_new_tokens': 512,
                    'do_sample': False,
                    'use_cache': True,
                    'pad_token_id': self.processor.tokenizer.eos_token_id,
                    'eos_token_id': self.processor.tokenizer.eos_token_id,
                }
                
                # Flash Attention 2 ç‰¹æ®Šä¼˜åŒ–
                if hasattr(self, 'attention_type') and self.attention_type == "flash_attention_2":
                    print("ğŸš€ Using Flash Attention 2 optimized generation...")
                    generation_kwargs.update({
                        'use_cache': False,  # Flash Attention æœ‰æ—¶ä¸ cache ä¸å…¼å®¹
                        'attention_mask': inputs.get('attention_mask', None),  # æ˜¾å¼ä¼ é€’ attention mask
                    })
                else:
                    print(f"ğŸ”§ Using {getattr(self, 'attention_type', 'default')} attention...")
                
                generation = self.model.generate(
                    **inputs, 
                    **generation_kwargs
                )
                generation = generation[0][input_len:]
            decoded = self.processor.decode(generation, skip_special_tokens=True)
            return decoded

        if self.type=="vllm":
            outputs = self.model.chat(messages, sampling_params=self.sampling_params)
            return outputs[0].outputs[0].text


    def inf_with_score(self, question: str, pictpath: str, max_new_tokens=512, num_beams=3):
        """
        é€šè¿‡ç»™å®šçš„é—®é¢˜å’Œå›¾ç‰‡è·¯å¾„ï¼Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆï¼Œå¹¶è¿”å›ç”Ÿæˆçš„ç­”æ¡ˆåŠå…¶ç½®ä¿¡åº¦å¾—åˆ†ã€‚
        :param question: ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œå­—ç¬¦ä¸²ç±»å‹ã€‚
        :param pictpath: å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„ï¼Œå­—ç¬¦ä¸²ç±»å‹ã€‚
        :param max_new_tokens: æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ä¸º128ã€‚
        :param num_beams: æŸæœç´¢çš„æ•°é‡ï¼Œé»˜è®¤ä¸º3ã€‚
        :return: åŒ…å«ç”Ÿæˆç­”æ¡ˆåŠå…¶å¯¹åº”åˆ†æ•°çš„åˆ—è¡¨ã€‚
        """

        if not os.path.exists(pictpath):
            raise FileNotFoundError(f"Image file not found: {pictpath}")

        # åˆ›å»ºæ¶ˆæ¯å†…å®¹
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": pictpath},
                    {"type": "text", "text": question}
                ]
            }
        ]

        if self.type == "hf":
            inputs = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image = Image.open(pictpath).convert("RGB")
            image.thumbnail((512, 512))  # ç¼©æ”¾ä»¥å‡å°‘æ˜¾å­˜å ç”¨

            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            inputs = self.processor(
                text=[inputs],
                images=image,
                return_tensors="pt"
            ).to(self.model.device).to(torch.bfloat16)  # ç¡®ä¿ä¸æ¨¡å‹è®¾å¤‡ä¸€è‡´

            print("Generating with beam search...")

            # ä½¿ç”¨æŸæœç´¢ç”Ÿæˆç­”æ¡ˆ
            output = self.model.generate(
                **inputs,
                do_sample=False,  # ä¸é‡‡æ ·ï¼Œä½¿ç”¨æŸæœç´¢
                max_new_tokens=max_new_tokens,
                num_beams=num_beams,
                num_return_sequences=num_beams,
                return_dict_in_generate=True,
                output_scores=True,
                use_cache=True
            )

            print("Generate finished.")

            # è§£ç ç”Ÿæˆçš„token
            generated_ids_trimmed = [out_ids[len(inputs.input_ids[0]):] for out_ids in output.sequences]
            output_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)

            # è®¡ç®—æ¯ä¸ªç”Ÿæˆåºåˆ—çš„å¾—åˆ†
            scores = output.sequences_scores.tolist()
            norm_scores = torch.softmax(output.sequences_scores, dim=0).tolist()

            # æ„å»ºç»“æœåˆ—è¡¨
            results = [
                {
                    "answer": output_text[i].strip(),
                    "score": scores[i],
                    "normalized_score": norm_scores[i]
                } for i in range(len(output_text))
            ]

            return results

        elif self.type == "vllm":
            pictpath="file://"+os.path.join("/root/autodl-tmp/RoG/qwen",pictpath)
            params = BeamSearchParams(beam_width=num_beams, max_tokens=max_new_tokens)
            outputs = self.model.beam_search(
                beam_width=num_beams,
                max_tokens=max_new_tokens,
                prompts=question,
                image= pictpath,
            )
            res=[]
            for out in outputs:
                res.append(out.sequence[0].text)
            print(res)
            return res
